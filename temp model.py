# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMLsTkwJado-1yLshxY6VBSwEf-IXS13

#install phase
"""

# !pip install yake

# !pip install fasttext

"""# imports"""

import json
import csv
import re
import numpy as np
import random
import networkx as nx
from matplotlib import pyplot as plt
from tqdm import tqdm
import pickle

import yake

from gensim.models.fasttext import load_facebook_vectors
from sklearn.decomposition import PCA,IncrementalPCA

SEED = 1



"""# cresci Keywords"""
'''
yake_extractor = yake.KeywordExtractor(lan="en", n=1, dedupLim=0.9)


tweet_keywords = {}
all_keywords = set()


## keyword extraction for cresci dataset
keywords_users_dic = {}
ts_id = 0
number_of_keywords = 10

def preprocess_tweet(text):
    text = re.sub(r"@\w+", "mention", text)
    
    # Replace hashtags with "hashtags"
    text = re.sub(r"#\w+", "hashtags", text)
    
    # Replace links with "link"
    text = re.sub(r"https?://\S+|www\.\S+", "link", text)
    
    # Remove non-English characters and punctuation
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove any extra spaces
    text = re.sub(r"\s+", " ", text).strip()
    
    return text


for folder_name in ['E13.csv', 'FSF.csv', 'INT.csv', 'TFP.csv', 'TWT.csv']:
    path = f"Fake_project_dataset_csv/{folder_name}/tweets.csv"
    
    try:
        with open(path, mode='r', encoding='utf-8', errors='ignore') as file:
            reader = csv.DictReader(file)
            for row in tqdm(reader, desc=f"Processing {folder_name}"):
                try:
                    user = row["user_id"]
                    text = preprocess_tweet(row["text"])

                    # Extract keywords
                    keywords = [kw[0] for kw in yake_extractor.extract_keywords(text)[:number_of_keywords]]

                    # Update dictionary
                    if user in keywords_users_dic:
                        keywords_users_dic[user].append({'t'+str(ts_id): keywords[:]})
                    else:
                        keywords_users_dic[user] = [{'t'+str(ts_id): keywords[:]}]

                    ts_id += 1
                    all_keywords.update(keywords[:])
                    
                    # Optional: Break every 5 iterations for debugging or testing
                    # if ts_id % 5 == 4:
                        # break

                except Exception as e:
                    print(f"Error processing row in {path}: {e}")
                    continue  # Skip problematic rows
                
        print(f"{path} is done")
    
    except FileNotFoundError:
        print(f"File {path} not found!")
    except Exception as e:
        print(f"Error processing file {path}: {e}")

with open('all_keywords.pickle', 'wb') as handle:
    pickle.dump(all_keywords, handle)

with open('keywords_users_dic.pickle', 'wb') as handle:
    pickle.dump(keywords_users_dic, handle)




# cresci embedding
"""#embedding"""

with open('all_keywords.pickle', 'rb') as handle:
    all_keywords = pickle.load(handle)



class FastTextEmbeddingModel:
    def __init__(self, fasttext_model, target_size=64):
        """
        Initialize the embedding model with a FastText model and PCA configuration.
        """
        self.model = fasttext_model
        self.target_size = target_size
        self.pca = None

    def _compute_word_embedding(self, word):
        """
        Fetch embedding for a single word. If the word is not in the model, return a zero vector.
        """
        if word in self.model:
            return self.model[word]
        else:
            return np.zeros(self.model.vector_size)

    def _compute_sentence_embedding(self, sentence):
        """
        Compute the embedding for a sentence by averaging the embeddings of its words.
        """
        words = sentence.split()  # Tokenize the sentence
        embeddings = [self._compute_word_embedding(word) for word in words if word in self.model]
        if embeddings:
            return np.mean(embeddings, axis=0)
        else:
            return np.zeros(self.model.vector_size)

    def fit_and_save(self, inputs, pca_save_path, batch_size=10000):
        """
        Fit PCA incrementally to handle large datasets and save the trained PCA model.
        """
        # Convert the inputs to a list if it's not already one
        if isinstance(inputs, set):
            inputs = list(inputs)

        self.pca = IncrementalPCA(n_components=self.target_size)

        # Process inputs in batches to reduce memory usage
        for i in range(0, len(inputs), batch_size):
            batch = inputs[i:i+batch_size]
            embeddings = [self._compute_embedding(text) for text in batch]
            self.pca.partial_fit(embeddings)

        # Save the PCA model
        with open(pca_save_path, 'wb') as f:
            pickle.dump(self.pca, f)
        print(f"PCA model trained and saved to {pca_save_path}")

    def load_pca(self, pca_load_path):
        """
        Load a pre-trained PCA model from a file.
        """
        with open(pca_load_path, 'rb') as f:
            self.pca = pickle.load(f)
        print(f"PCA model loaded from {pca_load_path}")

    def transform(self, inputs):
        """
        Transform inputs (list of words or sentences) into reduced-dimensional embeddings.
        """
        if not self.pca:
            raise ValueError("PCA is not fitted or loaded. Fit PCA or load a model before transforming.")
        
        embeddings = [self._compute_embedding(text) for text in inputs]
        reduced_embeddings = self.pca.transform(embeddings)
        return {text: reduced_embeddings[i] for i, text in enumerate(inputs)}

    def transform_dictionary(self, inputs):
        """
        Transform inputs (dictionary with keys and sentences/words as values) into reduced embeddings.
        """
        if not self.pca:
            raise ValueError("PCA is not fitted or loaded. Fit PCA or load a model before transforming.")
        
        embeddings = [self._compute_embedding(text) for text in inputs.values()]
        reduced_embeddings = self.pca.transform(embeddings)
        return {key: reduced_embeddings[i] for i, key in enumerate(inputs.keys())}
    
    def _compute_embedding(self, text):
        """
        Determine if the input is a sentence or a word and compute its embedding.
        """
        if " " in text:  # Sentence
            return self._compute_sentence_embedding(text)
        else:  # Word
            return self._compute_word_embedding(text)


fasttext_model = load_facebook_vectors('cc.en.300.bin')
embedding_model = FastTextEmbeddingModel(fasttext_model, target_size=32)
embedding_model.fit_and_save(all_keywords, pca_save_path="pca_model.pkl")
keywords_embedding = embedding_model.transform(all_keywords)

with open('word_embedding_dictionary.pickle', 'wb') as f:
    pickle.dump(keywords_embedding, f)
print("keywords_embedding done")




tweets_sentences_dic = {}


def preprocess_description(text):
    # Replace mentions with "mention"
    text = re.sub(r"@\w+", "mention", text)
    
    # Replace hashtags with "hashtags"
    text = re.sub(r"#\w+", "hashtags", text)
    
    # Replace links with "link"
    text = re.sub(r"https?://\S+|www\.\S+", "link", text)
    
    # Remove non-English characters and punctuation
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove any extra spaces
    text = re.sub(r"\s+", " ", text).strip()
    
    return text


# List of folders or files to process
folder_names = ['E13.csv', 'FSF.csv', 'INT.csv', 'TFP.csv', 'TWT.csv']

id_description_dict = {}

for folder_name in folder_names:
    path = f"Fake_project_dataset_csv/{folder_name}/users.csv"  # Assuming the file contains user data
    
    try:
        # Open the file for reading
        with open(path, mode='r', encoding='utf-8', errors='ignore') as file:
            reader = csv.DictReader(file)
            
            for row in tqdm(reader, desc=f"Processing {folder_name}"):
                try:
                    user_id = row["id"]
                    description = preprocess_description(row["description"]) if row["description"] else ""
                    
                    # Add to dictionary
                    id_description_dict[user_id] = description
                
                except KeyError as e:
                    print(f"Missing key in row for file {folder_name}: {e}")
                    continue
                except Exception as e:
                    print(f"Error processing row in {path}: {e}")
                    continue  # Skip problematic rows
            
        print(f"{path} is done")
    
    except FileNotFoundError:
        print(f"File {path} not found!")
    except Exception as e:
        print(f"Error processing file {path}: {e}")






embeddings_id_description_dic = embedding_model.transform_dictionary(id_description_dict)
with open('embeddings_id_description_dic.pickle', 'wb') as f:
    pickle.dump(embeddings_id_description_dic, f)
print("embeddings_id_description_dic done")



tweets_users_dic = {}
ts_id = 0  # Timestamp or unique identifier for tweets

for folder_name in ['E13.csv', 'FSF.csv', 'INT.csv', 'TFP.csv', 'TWT.csv']:
    path = f"Fake_project_dataset_csv/{folder_name}/tweets.csv"
    try:
        with open(path, mode='r', encoding='utf-8', errors='ignore') as file:
            reader = csv.DictReader(file)
            for row in tqdm(reader, desc=f"Processing {folder_name}"):
                try:
                    # Extract user ID and preprocess text
                    user = row["user_id"]
                    text = preprocess_description(row["text"])

                    # Compute embedding for the preprocessed text
                    embedding = embedding_model.transform([text])

                    # Update dictionary
                    if user in tweets_users_dic:
                        tweets_users_dic[user].append({'t' + str(ts_id):  list(embedding.values())[0]})
                    else:
                        tweets_users_dic[user] = [{'t' + str(ts_id):  list(embedding.values())[0]}]

                    
                    ts_id += 1  # Increment timestamp ID

                except KeyError as e:
                    print(f"Missing key in row for file {folder_name}: {e}")
                    continue
                except Exception as e:
                    print(f"Error processing row in {path}: {e}")
                    continue  # Skip problematic rows
            
        print(f"{path} is done")
    
    except FileNotFoundError:
        print(f"File {path} not found!")
    except Exception as e:
        print(f"Error processing file {path}: {e}")


# def print_sample(dictionary, sample_size=3):
#     if dictionary:
#         sample = {k: dictionary[k] for k in list(dictionary)[:sample_size]}
#         print(f"Sample from dictionary ({len(dictionary)} entries):\n{sample}\n")
# print_sample(tweets_users_dic, sample_size=5) 

# Optional: Save the dictionary for future use
output_path = "tweets_users_dic.pkl"
try:
    with open(output_path, 'wb') as f:
        pickle.dump(tweets_users_dic, f)
    print(f"Dictionary saved to {output_path}")
except Exception as e:
    print(f"Error saving dictionary: {e}")

'''

'''
with open('tweets_users_dic.pkl', 'rb') as handle:
    tweets_users_dic = pickle.load(handle)

tweet_id_embedding_dic = {}
for row in tqdm(tweets_users_dic.keys()):
    for subrow in tweets_users_dic[row]:
        k = list(subrow.keys())[0]
        tweet_id_embedding_dic[k] = subrow[k]

with open('tweet_id_embedding_dic.pickle', 'wb') as handle:
    pickle.dump(tweet_id_embedding_dic, handle)
'''

    

"""# creat graph"""
'''
with open('keywords_users_dic.pickle', 'rb') as handle:
    keywords_users_dic = pickle.load(handle)



with open('tweets_users_dic.pkl', 'rb') as handle:
    tweets_users_dic = pickle.load(handle)

with open('embeddings_id_description_dic.pickle', 'rb') as handle:
    embeddings_id_description_dic = pickle.load(handle)

with open('word_embedding_dictionary.pickle', 'rb') as handle:
    word_embedding_dictionary = pickle.load(handle)

with open('tweet_id_embedding_dic.pickle', 'rb') as handle:
    tweet_id_embedding_dic = pickle.load(handle)






# def print_sample(dictionary, sample_size=3):
#     if dictionary:
#         sample = {k: dictionary[k] for k in list(dictionary)[:sample_size]}
#         print(f"Sample from dictionary ({len(dictionary)} entries):\n{sample}\n")
#     else:
#         print("Dictionary is not available.\n")

# Print samples from each dictionary
# print("Keywords Users Dictionary:")
# print_sample(tweets_users_dic)

# print("Embeddings ID Description Dictionary:")
# print_sample(embeddings_id_description_dic)

# print("Embeddings keywords_embedding:")
# print_sample(word_embedding_dictionary)




all_keywords = list(word_embedding_dictionary.keys())
all_users = list(embeddings_id_description_dic.keys())

temp = [int(i[1:]) for i in  list(tweet_id_embedding_dic.keys())]
# print(max(temp),min(temp),len(temp))
last_tweet_number = max(temp)

HG = nx.MultiDiGraph()

# Add nodes of different types include 'user', 'keyword', 'tweet'
for node in list(all_users):
    HG.add_node(node, node_type="user")
for node in tqdm(all_keywords):
    HG.add_node(node, node_type="keyword")

print(' ----------------------          ',"251603425" in HG.nodes)

print("last_tweet_number")
for row in tqdm(range(0,last_tweet_number+1)):
    HG.add_node('t'+str(row), node_type="tweet")

print(' ----------------------          ',"251603425" in HG.nodes)

print("nodes added")

# Add directed edges with types include 'following', 'follower', 'friend', 'haskeyword', 'keywordintweet', 'hastweet', 'tweetedby'
for folder_name in ['E13.csv', 'FSF.csv', 'INT.csv', 'TFP.csv', 'TWT.csv']:

    path = f"Fake_project_dataset_csv/{folder_name}/followers.csv"
    with open(path, 'r') as csv_file:
        reader = csv.reader(csv_file)
        next(reader)  # Skip the header line
        set_all_user = set(all_users)
        for edge in tqdm(reader):
            if edge[0] not in  set_all_user:
                HG.add_node(edge[0], node_type="user")
                set_all_user.add(edge[0])
            if edge[1] not in  set_all_user:
                HG.add_node(edge[1], node_type="user")
                set_all_user.add(edge[1])

            HG.add_edge(edge[0], edge[1], edge_type="follower")
            HG.add_edge(edge[1], edge[0], edge_type="following")
print(' ----------------------          ',"251603425" in HG.nodes)

for folder_name in ['E13.csv', 'FSF.csv', 'INT.csv', 'TFP.csv', 'TWT.csv']:
    path = f"Fake_project_dataset_csv/{folder_name}/friends.csv"
    with open(path, 'r') as csv_file:
        reader = csv.reader(csv_file)
        next(reader)  # Skip the header line
        for edge in reader:
            if edge[0] not in  set_all_user:
                HG.add_node(edge[0], node_type="user")
                set_all_user.add(edge[0])
            if edge[1] not in  set_all_user:
                HG.add_node(edge[1], node_type="user")
                set_all_user.add(edge[1])
            HG.add_edge(edge[0], edge[1], edge_type="friend")
            HG.add_edge(edge[1], edge[0], edge_type="friend")

print(' ----------------------          ',"251603425" in HG.nodes)

nodes = set(list(HG.nodes()))



for user in tqdm(all_users):

    try:
        tweets = tweets_users_dic[user]
        for row in tweets:
            tweet = list(row.keys())[0]
            if user in nodes and tweet in nodes:
                HG.add_edge(user, tweet, edge_type="hastweet")
                HG.add_edge(tweet, user, edge_type="tweetedby")          
            else:
                print(user,tweet, user in nodes , tweet in nodes)
                break

    except:
        print("no tweet by ",user)
        
print(' ----------------------          ',"251603425" in HG.nodes)



for user in tqdm(keywords_users_dic.keys()):
    for row in keywords_users_dic[user]:
        tweet = list(row.keys())[0]
        keywords = row[tweet]
        for kw in keywords:
            if kw in nodes and tweet in nodes:  
                HG.add_edge(kw, tweet, edge_type="keywordintweet")
                HG.add_edge(tweet, kw, edge_type="haskeyword")
            else:
                print('sssssssss   ',kw,tweet, kw in nodes , tweet in nodes)
                break

print(' ----------------------          ',"251603425" in HG.nodes)

with open('HG0.pickle', 'wb') as handle:
    pickle.dump(HG, handle)





 # convertion in dictionary of tweets, remove user keys and now it is just tweetid and tweet

# with open('tweets_users_dic.pkl', 'rb') as handle:
#     tweets_users_dic = pickle.load(handle)
# print("read twt")



'''




'''

with open('tweet_id_embedding_dic.pickle', 'rb') as handle:
    tweet_id_embedding_dic = pickle.load(handle)

with open('embeddings_id_description_dic.pickle', 'rb') as handle:
    embeddings_id_description_dic = pickle.load(handle)
print("read desc")

with open('tweets_users_dic.pkl', 'rb') as handle:
    tweets_users_dic = pickle.load(handle)
print("read twt")

with open('word_embedding_dictionary.pickle', 'rb') as handle:
    word_embedding_dictionary = pickle.load(handle)
print("read wrd")

with open('HG0.pickle', 'rb') as handle:
    HG = pickle.load(handle)
print("read gh")


all_users = [int(i) for i in list(embeddings_id_description_dic.keys())]
print(len(all_users),min(all_users),max(all_users))






# def print_sample(dictionary, sample_size=1):
#     sample = {k: dictionary[k] for k in list(dictionary)[:sample_size]}
#     print(f"Sample from dictionary ({len(dictionary)} entries):\n{sample}\n")
   


# node_types = set()
# for n, data in HG.nodes(data=True):
#     # Assumes node data includes a 'type' attribute, e.g. data['type']
#     if 'type' in data:
#         node_types.add(data['type'])

# print("Node types found in the graph:", node_types)

# # 2. Print a small sample of nodes for each type
# for nt in node_types:
#     # collect nodes of this type
#     nodes_of_type = [n for n, d in HG.nodes(data=True) if d.get('type') == nt]
#     sample_nodes = nodes_of_type[:5]  # take up to 5 nodes as a sample
#     print(f"Sample nodes of type '{nt}':", sample_nodes)

# # 3. Extract the unique edge types
# edge_types = set()
# for u, v, key, data in HG.edges(keys=True, data=True):
#     # Assumes edge data includes a 'type' attribute, e.g. data['type']
#     if 'type' in data:
#         edge_types.add(data['type'])

# print("\nEdge types found in the graph:", edge_types)

# # 4. Print a small sample of edges for each type
# for et in edge_types:
#     # collect edges of this type
#     edges_of_type = [(u, v, k) for u, v, k, d in HG.edges(keys=True, data=True) if d.get('type') == et]
#     sample_edges = edges_of_type[:5]  # take up to 5 edges as a sample
#     print(f"Sample edges of type '{et}':", sample_edges)

# print_sample(embeddings_id_description_dic)
# print_sample(tweets_users_dic)
# print_sample(word_embedding_dictionary)


nodes = set(list(HG.nodes()))

print(' ----------------------          ',"12" in HG.nodes)



nodes_without_description = []

# Assign embeddings to keyword nodes
for node, data in tqdm(HG.nodes(data=True)):

    if data["node_type"] == "keyword":
        keyword = node
        data["embedding"] = word_embedding_dictionary[keyword]
    if data["node_type"] == "user":
        user = node
        try:
            data["embedding"] = embeddings_id_description_dic[user]
        except:
            nodes_without_description.append((node,data))
    if data["node_type"] == "tweet":
        tweet = node
        data["embedding"] = tweet_id_embedding_dic[tweet]


print("terreterrtete")


embedding_dim = len(next(iter(word_embedding_dictionary.values())))

for node, data in tqdm(nodes_without_description):
    neighbors = list(HG.neighbors(node))
    
    neighbor_embeddings = []
    for neigh in neighbors:
        neigh_data = HG.nodes[neigh]
        if "embedding" in neigh_data:
            neighbor_embeddings.append(neigh_data["embedding"])
    
    if len(neighbor_embeddings) > 0:
        # Compute average embedding of neighbors
        avg_embedding = np.mean(neighbor_embeddings, axis=0)
        
        # Add small random noise to the embedding to make it slightly distinct
        noise = np.random.normal(loc=0, scale=0.01, size=avg_embedding.shape)
        data["embedding"] = avg_embedding + noise
  
    else:
        # If no neighbors have embeddings, default to a random embedding
        # (You can also choose another strategy here)
        data["embedding"] = np.random.normal(loc=0, scale=0.1, size=embedding_dim)
 
# At this point, all nodes in nodes_without_description should have embeddings

with open('HG2.pickle', 'wb') as handle:
    pickle.dump(HG, handle)
'''

with open('HG2.pickle', 'rb') as handle:
    HG = pickle.load(handle)
print("read gh")

##### RAM intensive

# def relabel_heterogeneous_graph(HG):
#     # Extract nodes and create a mapping
#     old_to_new = {node: idx for idx, node in enumerate(HG.nodes())}
#     new_to_old = {idx: node for node, idx in old_to_new.items()}
    
#     # Relabel nodes in the graph
#     HG_relabelled = nx.relabel_nodes(HG, old_to_new)
    
#     return HG_relabelled, old_to_new, new_to_old

# HG, old_to_new, new_to_old = relabel_heterogeneous_graph(HG)

'''
### RAM problem still exist
def relabel_heterogeneous_graph(HG):
    # Get the original nodes in order
    old_nodes = list(HG.nodes())
    
    # Convert all node labels to integers from 0 to n-1
    HG_relabelled = nx.convert_node_labels_to_integers(HG, first_label=0)
    
    # Create mapping dictionaries
    # convert_node_labels_to_integers assigns new labels based on the
    # order in old_nodes, so old_nodes[i] is mapped to i.
    old_to_new = {old_node: i for i, old_node in enumerate(old_nodes)}
    new_to_old = {i: old_node for i, old_node in enumerate(old_nodes)}
    
    return HG_relabelled, old_to_new, new_to_old

# Usage
HG, old_to_new, new_to_old = relabel_heterogeneous_graph(HG)
'''

import networkx as nx
import numpy as np
import json
import csv
import tempfile

def externalize_graph(HG, embedding_attr='embedding', node_attr_file='node_attrs.jsonl', 
                      edge_attr_file='edge_attrs.jsonl', emb_file_prefix='node_emb'):
    """
    Convert nodes to integers and store all features (node and edge attributes) externally.
    
    Parameters
    ----------
    HG : nx.Graph or nx.DiGraph or nx.MultiGraph or nx.MultiDiGraph
        The original heterogeneous graph with arbitrary node labels and attributes.
    embedding_attr : str
        The node attribute name where embeddings are stored (if any).
    node_attr_file : str
        Path to a JSON lines file where non-embedding node attributes will be stored.
    edge_attr_file : str
        Path to a JSON lines file where edge attributes will be stored.
    emb_file_prefix : str
        Prefix for embedding-related files. We'll store embeddings in memory-mapped format.

    Returns
    -------
    HG_int : nx.Graph (or the same type as HG)
        A new graph with integer-labeled nodes (0 to n-1) and edges, but without large attributes.
    mapping_files : dict
        A dictionary containing paths to the various external files created.
    """

    # Step 1: Create old-to-new mapping for nodes
    old_nodes = list(HG.nodes())
    old_to_new = {old_node: i for i, old_node in enumerate(old_nodes)}
    new_to_old = {i: old_node for old_node, i in old_to_new.items()}
    
    # Determine embedding dimension if embeddings are present
    emb_dim = None
    for n in old_nodes:
        if embedding_attr in HG.nodes[n]:
            emb_dim = len(HG.nodes[n][embedding_attr])
            break
    
    # Create memory-mapped file for embeddings if applicable
    emb_file_name = None
    emb_array = None
    if emb_dim is not None:
        # Create a memory-mapped array for node embeddings
        emb_tmp = tempfile.NamedTemporaryFile(prefix=emb_file_prefix, delete=False)
        emb_file_name = emb_tmp.name
        emb_tmp.close()
        emb_array = np.memmap(emb_file_name, dtype='float32', mode='w+', shape=(len(old_nodes), emb_dim))
    
    # Step 2: Create an integer-labeled graph
    HG_int = type(HG)()
    HG_int.add_nodes_from(range(len(old_nodes)))
    
    # If it's a multi-graph, we need to handle keys as well
    is_multigraph = HG.is_multigraph()
    
    # Add edges with integer labels
    if is_multigraph:
        for u, v, k, data in HG.edges(data=True, keys=True):
            HG_int.add_edge(old_to_new[u], old_to_new[v], key=k)
    else:
        for u, v, data in HG.edges(data=True):
            HG_int.add_edge(old_to_new[u], old_to_new[v])
    
    # Step 3: Externalize node attributes
    # We'll store all non-embedding attributes in node_attrs.jsonl
    # Each line: {"node_id": int, "attributes": {...}}
    with open(node_attr_file, 'w', encoding='utf-8') as na_file:
        for i, old_node in enumerate(old_nodes):
            attrs = HG.nodes[old_node].copy()
            emb = attrs.pop(embedding_attr, None)  # remove embedding if present
            # Store embedding in memmap if available
            if emb is not None and emb_array is not None:
                emb_array[i] = emb
            
            # Write remaining attributes as JSON line
            na_record = {
                "node_id": i,
                "attributes": attrs
            }
            na_file.write(json.dumps(na_record) + "\n")
    
    if emb_array is not None:
        emb_array.flush()
    
    # Step 4: Externalize edge attributes
    # Store edge attributes in edge_attrs.jsonl, 
    # each line: {"src": int, "dst": int, "key": optional int, "attributes": {...}}
    with open(edge_attr_file, 'w', encoding='utf-8') as ea_file:
        if is_multigraph:
            for u, v, k, data in HG.edges(data=True, keys=True):
                ea_record = {
                    "src": old_to_new[u],
                    "dst": old_to_new[v],
                    "key": k,
                    "attributes": data
                }
                ea_file.write(json.dumps(ea_record) + "\n")
        else:
            for u, v, data in HG.edges(data=True):
                ea_record = {
                    "src": old_to_new[u],
                    "dst": old_to_new[v],
                    "attributes": data
                }
                ea_file.write(json.dumps(ea_record) + "\n")
    
    # Step 5: Store mappings and metadata
    mapping_files = {}
    
    # Save old_to_new and new_to_old mappings
    with open("node_mapping.json", 'w', encoding='utf-8') as f:
        json.dump({"old_to_new": {str(k): v for k,v in old_to_new.items()},
                   "new_to_old": {str(k): v for k,v in new_to_old.items()}}, f)
    mapping_files["node_mapping"] = "node_mapping.json"
    
    # Save embedding metadata if applicable
    if emb_file_name is not None:
        with open("embedding_metadata.json", 'w', encoding='utf-8') as f:
            json.dump({"embedding_file": emb_file_name, "dimension": emb_dim}, f)
        mapping_files["embedding_metadata"] = "embedding_metadata.json"
    
    mapping_files["node_attrs"] = node_attr_file
    mapping_files["edge_attrs"] = edge_attr_file
    
    return HG_int, mapping_files

def load_integer_graph(node_mapping_file='node_mapping.json',
                       node_attr_file='node_attrs.jsonl',
                       edge_attr_file='edge_attrs.jsonl',
                       embedding_metadata_file='embedding_metadata.json'):
    """
    Load the integer-labeled graph and set up external attribute access.
    """
    # Load mappings
    with open(node_mapping_file, 'r', encoding='utf-8') as f:
        mapping_data = json.load(f)
    old_to_new = {k: v for k, v in mapping_data["old_to_new"].items()}
    new_to_old = {int(k): v for k, v in mapping_data["new_to_old"].items()}
    
    # Determine number of nodes
    n = len(new_to_old)
    
    # Create the graph
    G_int = nx.DiGraph()  # or the appropriate type (you might store the original graph type in a metadata file)
    G_int.add_nodes_from(range(n))
    
    # Load edges
    # Since we stored src, dst, and possibly key in edge_attrs.jsonl, we can rebuild the edge structure
    # without attributes first.
    is_multigraph = False
    edges_temp = []
    with open(edge_attr_file, 'r', encoding='utf-8') as ea_file:
        for line in ea_file:
            rec = json.loads(line)
            src = rec["src"]
            dst = rec["dst"]
            if "key" in rec:
                # Multi-graph
                is_multigraph = True
                edges_temp.append((src, dst, rec["key"]))
            else:
                edges_temp.append((src, dst))
    
    if is_multigraph:
        G_int = nx.MultiDiGraph()
        G_int.add_nodes_from(range(n))
        for e in edges_temp:
            G_int.add_edge(*e)
    else:
        # It's a simple graph
        G_int = nx.DiGraph()
        G_int.add_nodes_from(range(n))
        G_int.add_edges_from(edges_temp)
    
    # Load embedding metadata if available
    embedding_file = None
    emb_dim = None
    if embedding_metadata_file is not None:
        try:
            with open(embedding_metadata_file, 'r', encoding='utf-8') as f:
                emb_meta = json.load(f)
            embedding_file = emb_meta["embedding_file"]
            emb_dim = emb_meta["dimension"]
        except FileNotFoundError:
            pass
    
    # Provide a convenient access pattern for attributes:
    # We'll create a helper class that can fetch node attributes and embeddings on-demand.
    # Node attributes (except embeddings) are in node_attrs.jsonl, keyed by node_id.
    # We will load them into a dictionary for quick access. If this file is huge, consider a database or on-demand parsing.
    
    node_attrs_dict = {}
    with open(node_attr_file, 'r', encoding='utf-8') as na_file:
        for line in na_file:
            rec = json.loads(line)
            node_id = rec["node_id"]
            node_attrs_dict[node_id] = rec["attributes"]
    
    # Edge attributes are similarly stored in edge_attrs.jsonl
    # For large graphs, you might prefer a database or on-demand access rather than loading into memory.
    edge_attrs_list = []
    with open(edge_attr_file, 'r', encoding='utf-8') as ea_file:
        for line in ea_file:
            rec = json.loads(line)
            edge_attrs_list.append(rec)
    
    # Create a memory-mapped array for embeddings if applicable
    emb_array = None
    if embedding_file is not None and emb_dim is not None:
        emb_array = np.memmap(embedding_file, dtype='float32', mode='r', shape=(n, emb_dim))
    
    # Return the graph and accessors
    return G_int, new_to_old, emb_array, node_attrs_dict, edge_attrs_list


HG_int, files = externalize_graph(HG)

# Later, you can load it back:
G_int, new_to_old, emb_array, node_attrs_dict, edge_attrs_list = load_integer_graph()

# Access attributes on-demand:
node_id = 0  # an integer-labeled node
node_attrs = node_attrs_dict[node_id]  # Access node's stored attributes
if emb_array is not None:
    node_embedding = emb_array[node_id]  # Access the node's embedding on demand

# Edges and their attributes:
# For each edge record in edge_attrs_list, you can match it to G_int's edges:
for e_rec in edge_attrs_list:
    src = e_rec["src"]
    dst = e_rec["dst"]
    e_attrs = e_rec["attributes"]



# with open('HG3.pickle', 'wb') as handle:
#     pickle.dump(HG, handle)

# with open('old_to_new.pickle', 'wb') as handle:
#     pickle.dump(old_to_new, handle)

# with open('new_to_old.pickle', 'wb') as handle:
#     pickle.dump(new_to_old, handle)

print("kheilyyy saved")


'''
with open('HG.pickle', 'rb') as handle:
    HG = pickle.load(handle)

def update_heterogeneous_graph(HG, node_mapping):
    # Create a new graph to hold updated nodes
    updated_HG = nx.Graph() if isinstance(HG, nx.Graph) else nx.DiGraph()

    # Update nodes and their attributes
    for node, data in tqdm(HG.nodes(data=True)):
        new_node = node_mapping[node]
        updated_HG.add_node(new_node, **data)  # Copy node attributes to the new graph

    # Add edges with the new node identifiers
    for u, v, data in tqdm(HG.edges(data=True)):
        updated_HG.add_edge(node_mapping[u], node_mapping[v], **data)

    return updated_HG

node_mapping = {node: idx for idx, node in tqdm(enumerate(HG.nodes))}
HG = update_heterogeneous_graph(HG, node_mapping)

with open('HG.pickle', 'wb') as handle:
    pickle.dump(HG, handle)
'''




'''

with open('HG.pickle', 'rb') as handle:
    HG = pickle.load(handle)
    
    

import torch
from torch_geometric.nn import HGTConv
from torch_geometric.data import HeteroData


# 'user', 'keyword', 'tweet'
# 'follow', 'haskeyword', 'keywordintweet', 'hastweet', 'tweetedby'


# Initialize HeteroData object
# data = HeteroData()

# from collections import defaultdict
# node_features = defaultdict(list)  # Temporary storage for node features

# # Collect node embeddings grouped by node_type
# for node, attributes in HG.nodes(data=True):
#     node_type = attributes.get('node_type')  # Get node type
#     embedding = attributes.get('embedding')  # Get embedding

#     if node_type is None or embedding is None:
#         print(f"Node {node} is missing 'node_type' or 'embedding'. Skipping.")
#         continue

#     node_features[node_type].append(embedding)  # Group embeddings by node_type

# # Convert grouped embeddings to tensors and assign to HeteroData
# for node_type, embeddings in node_features.items():
#     # Convert list of embeddings to tensor
#     data[node_type].x = torch.tensor(embeddings, dtype=torch.float)


# # Convert node features (embeddings) to tensor
# for node_type in data.node_types:
#     data[node_type].x = torch.tensor(data[node_type].x, dtype=torch.float)

# # Add edges to HeteroData based on their type
# for source, target, attributes in HG.edges(data=True):
#     edge_type = attributes['edge_type']
#     src_type = HG.nodes[source]['node_type']
#     tgt_type = HG.nodes[target]['node_type']

#     # Initialize the edge_index for this edge type if it doesn't exist
#     if (src_type, edge_type, tgt_type) not in data.edge_types:
#         data[(src_type, edge_type, tgt_type)].edge_index = torch.empty((2, 0), dtype=torch.long)

#     # Append the edge indices to the existing tensor
#     current_edges = data[(src_type, edge_type, tgt_type)].edge_index
#     new_edges = torch.tensor([[source], [target]], dtype=torch.long)
#     data[(src_type, edge_type, tgt_type)].edge_index = torch.cat([current_edges, new_edges], dim=1)

# # The HeteroData object is now populated with tensors
# print(data)

# # Verify the types of edge_index
# # print(type(data['user', 'follow', 'user'].edge_index))  # Should be torch.Tensor
# # print(type(data['user', 'follow', 'user'].edge_index[0]))


# with open('data.pickle', 'wb') as handle:
#     pickle.dump(data, handle)


'''


'''

# with open('data.pickle', 'rb') as handle:
#     data = pickle.load(handle)
    
# print(data)
# # user_node_data = data['user']

# import torch
# import torch.nn.functional as F
# from torch_geometric.nn import HeteroConv, GATConv, Linear
# from sklearn.model_selection import train_test_split


# class HeteroGNNWithTypes(torch.nn.Module):
#     def __init__(self, metadata, in_channels, hidden_channels, out_channels, num_edge_types, num_node_types, heads=1):
#         super().__init__()
#         self.metadata = metadata

#         # Initialize the GNN layers for edge types
#         self.conv1 = HeteroConv({
#             edge_type: GATConv(
#                 in_channels=in_channels,
#                 out_channels=hidden_channels,
#                 heads=heads,
#                 add_self_loops = False
#             )
#             for edge_type in metadata[1]
#         }, aggr='mean')  # Aggregate results across edge types

#         self.conv2 = HeteroConv({
#             edge_type: GATConv(
#                 in_channels=hidden_channels * heads,
#                 out_channels=hidden_channels,
#                 heads=heads,
#                 add_self_loops = False
#             )
#             for edge_type in metadata[1]
#         }, aggr='mean')

#         # Final classifier for node types
#         self.classifiers = torch.nn.ModuleDict({
#             node_type: Linear(hidden_channels * heads, out_channels)
#             for node_type in metadata[0]
#         })

#         # Embeddings for node and edge types
#         self.node_type_embed = torch.nn.Embedding(num_node_types, in_channels)
#         self.edge_type_embed = torch.nn.Embedding(num_edge_types, hidden_channels)

#     def forward(self, data):
#         x_dict, edge_index_dict = {}, {}

#         # Prepare x_dict and add node type embeddings
#         for node_type in data.node_types:
#             x_dict[node_type] = data[node_type].x
#             node_type_ids = torch.full((x_dict[node_type].size(0),), list(data.node_types).index(node_type), dtype=torch.long)
#             x_dict[node_type] += self.node_type_embed(node_type_ids)

#         # Prepare edge_index_dict without self-loops
#         for edge_type in data.edge_types:
#             edge_index = data[edge_type].edge_index
#             edge_index_dict[edge_type] = edge_index  # Use edge_index as-is without adding self-loops



#         node_offsets = {}
#         current_offset = 0
#         for node_type in metadata[0]:
#             node_offsets[node_type] = current_offset
#             current_offset += data[node_type]['x'].size(0)

#         # Adjust edge indices
#         adjusted_edge_index_dict = {}
#         for edge_type in data.edge_types:
#             src_type, _, tgt_type = edge_type
#             edge_index = data[edge_type]['edge_index']

#             # Adjust indices for source and target nodes
#             adjusted_edge_index = edge_index.clone()  # Copy the original edge_index
#             adjusted_edge_index[0] -= node_offsets[src_type]  # Adjust source node indices
#             adjusted_edge_index[1] -= node_offsets[tgt_type]  # Adjust target node indices

#             # Store the adjusted edge index
#             adjusted_edge_index_dict[edge_type] = adjusted_edge_index


#         # Pass through first HeteroConv layer
#         x_dict = self.conv1(x_dict, adjusted_edge_index_dict)
#         x_dict = {key: F.elu(x) for key, x in x_dict.items()}

#         # Pass through second HeteroConv layer
#         x_dict = self.conv2(x_dict, adjusted_edge_index_dict)
#         x_dict = {key: F.elu(x) for key, x in x_dict.items()}

#         # Apply classifiers for each node type
#         out_dict = {}
#         for node_type, x in x_dict.items():
#             out_dict[node_type] = F.softmax(self.classifiers[node_type](x), dim=1)

#         return out_dict


# # Metadata for the heterogeneous graph
# metadata = (
#     ['user', 'keyword', 'tweet'],  # Node types
#     [('user', 'follow', 'user'), ('user', 'hastweet', 'tweet'), ('tweet', 'tweetedby', 'user'), ('keyword', 'keywordintweet', 'tweet'), ('tweet', 'haskeyword', 'keyword') ]  # Edge types
# )

# # Initialize the model
# model = HeteroGNNWithTypes(
#     metadata=metadata,
#     in_channels=16,  # Match input feature size
#     hidden_channels=32,  # Hidden size
#     out_channels=2,  # Binary classification
#     num_edge_types=len(metadata[1]),  # Number of edge types
#     num_node_types=len(metadata[0]),  # Number of node types
#     heads=2  # Multi-head attention
# )

# # Forward pass
# output = model(data)
# # Print output
# for node_type, logits in output.items():
#     print(f"{node_type} output shape: {logits.shape}")

# # Example data structure for node labels
# # Replace with your actual node labels for classification
# data['user'].y = torch.randint(0, 2, (100,))  # Random binary labels for users

# # Split indices for training and testing
# train_mask, test_mask = train_test_split(
#     torch.arange(data['user'].x.size(0)), test_size=0.2, random_state=42
# )

# # Create train/test masks
# data['user'].train_mask = torch.zeros(data['user'].x.size(0), dtype=torch.bool)
# data['user'].test_mask = torch.zeros(data['user'].x.size(0), dtype=torch.bool)

# data['user'].train_mask[train_mask] = True
# data['user'].test_mask[test_mask] = True

# # Define loss and optimizer
# criterion = torch.nn.CrossEntropyLoss()  # Suitable for multi-class classification
# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)

# # Training loop
# def train():
#     model.train()  # Set model to training mode
#     optimizer.zero_grad()  # Clear previous gradients

#     # Forward pass
#     out_dict = model(data)
#     out = out_dict['user']  # Focus on the target node type

#     # Compute loss only for training nodes
#     train_mask = data['user'].train_mask
#     loss = criterion(out[train_mask], data['user'].y[train_mask])

#     # Backward pass and optimization
#     loss.backward(retain_graph=True )  # Compute gradients
#     optimizer.step()  # Update model parameters

#     return loss.item()

# # Testing function
# @torch.no_grad()
# def test():
#     model.eval()  # Set model to evaluation mode

#     # Forward pass
#     out_dict = model(data)
#     out = out_dict['user']

#     # Get predictions
#     pred = out.argmax(dim=1)  # Predicted class labels

#     # Evaluate accuracy on train and test sets
#     train_mask = data['user'].train_mask
#     test_mask = data['user'].test_mask

#     train_acc = (pred[train_mask] == data['user'].y[train_mask]).sum().item() / train_mask.sum().item()
#     test_acc = (pred[test_mask] == data['user'].y[test_mask]).sum().item() / test_mask.sum().item()

#     return train_acc, test_acc

# # Training loop
# num_epochs = 100
# for epoch in range(1, num_epochs + 1):
#     loss = train()
#     train_acc, test_acc = test()

#     if epoch%20==1 or epoch == num_epochs:
#         print(f"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")
'''


'''
with open('HG.pickle', 'rb') as handle:
    hg = pickle.load(handle)
    
    
import torch
import torch.nn.functional as F
from torch_geometric.nn import HeteroConv, GATConv, Linear
from sklearn.model_selection import train_test_split
from collections import defaultdict

# Assume HG is your initial heterogeneous graph structure (for example, a networkx graph)
# HG nodes have attributes: 'node_type', 'embedding'
# HG edges have attributes: 'edge_type'

#########################
# Construct a HeteroData object from HG
#########################


node_features = defaultdict(list)  # Temporary storage for node features

# Collect node embeddings grouped by node_type
for node, attributes in HG.nodes(data=True):
    node_type = attributes.get('node_type')
    embedding = attributes.get('embedding')

    if node_type is None or embedding is None:
        print(f"Node {node} is missing 'node_type' or 'embedding'. Skipping.")
        continue

    node_features[node_type].append(embedding)

# Convert grouped embeddings to tensors and assign to HeteroData
for node_type, embeddings in node_features.items():
    hg[node_type].x = torch.tensor(embeddings, dtype=torch.float)

# Add edges to HeteroData based on their type
for source, target, attributes in HG.edges(data=True):
    edge_type = attributes['edge_type']
    src_type = HG.nodes[source]['node_type']
    tgt_type = HG.nodes[target]['node_type']

    if (src_type, edge_type, tgt_type) not in hg.edge_types:
        hg[(src_type, edge_type, tgt_type)].edge_index = torch.empty((2, 0), dtype=torch.long)

    current_edges = hg[(src_type, edge_type, tgt_type)].edge_index
    new_edges = torch.tensor([[source], [target]], dtype=torch.long)
    hg[(src_type, edge_type, tgt_type)].edge_index = torch.cat([current_edges, new_edges], dim=1)

print("Constructed HeteroData:")
print(hg)

#########################
# Define the model
#########################
class HeteroGNNWithTypes(torch.nn.Module):
    def __init__(self, metadata, in_channels, hidden_channels, out_channels, num_edge_types, num_node_types, heads=1):
        super().__init__()
        self.metadata = metadata

        # Initialize the GNN layers for edge types
        self.conv1 = HeteroConv({
            edge_type: GATConv(
                in_channels=in_channels,
                out_channels=hidden_channels,
                heads=heads,
                add_self_loops=False
            )
            for edge_type in metadata[1]
        }, aggr='mean')  # Aggregate results across edge types

        self.conv2 = HeteroConv({
            edge_type: GATConv(
                in_channels=hidden_channels * heads,
                out_channels=hidden_channels,
                heads=heads,
                add_self_loops=False
            )
            for edge_type in metadata[1]
        }, aggr='mean')

        # Final classifier for node types
        self.classifiers = torch.nn.ModuleDict({
            node_type: Linear(hidden_channels * heads, out_channels)
            for node_type in metadata[0]
        })

        # Embeddings for node and edge types
        self.node_type_embed = torch.nn.Embedding(num_node_types, in_channels)
        self.edge_type_embed = torch.nn.Embedding(num_edge_types, hidden_channels)

    def forward(self, hg):
        x_dict = {}
        
        # Prepare x_dict and add node type embeddings
        for i, node_type in enumerate(hg.node_types):
            x_dict[node_type] = hg[node_type].x
            node_type_ids = torch.full((x_dict[node_type].size(0),), i, dtype=torch.long)
            x_dict[node_type] += self.node_type_embed(node_type_ids)

        # Compute offsets for indexing
        node_offsets = {}
        current_offset = 0
        for node_type in self.metadata[0]:
            node_offsets[node_type] = current_offset
            current_offset += hg[node_type].x.size(0)

        # Adjust edge indices
        adjusted_edge_index_dict = {}
        for edge_type in hg.edge_types:
            src_type, rel_type, dst_type = edge_type
            edge_index = hg[edge_type].edge_index
            # Adjust indices for source and target nodes
            adjusted_edge_index = edge_index.clone()
            adjusted_edge_index[0] -= node_offsets[src_type]
            adjusted_edge_index[1] -= node_offsets[dst_type]

            adjusted_edge_index_dict[edge_type] = adjusted_edge_index

        # Pass through first HeteroConv layer
        x_dict = self.conv1(x_dict, adjusted_edge_index_dict)
        x_dict = {key: F.elu(x) for key, x in x_dict.items()}

        # Pass through second HeteroConv layer
        x_dict = self.conv2(x_dict, adjusted_edge_index_dict)
        x_dict = {key: F.elu(x) for key, x in x_dict.items()}

        # Apply classifiers for each node type
        out_dict = {}
        for node_type, x in x_dict.items():
            out_dict[node_type] = F.softmax(self.classifiers[node_type](x), dim=1)

        return out_dict

#########################
# Set up metadata, model, labels, and masks
#########################
metadata = (
    list(hg.node_types),
    list(hg.edge_types)
)

model = HeteroGNNWithTypes(
    metadata=metadata,
    in_channels=16,  # Adjust as needed for your embeddings
    hidden_channels=32,
    out_channels=2,
    num_edge_types=len(metadata[1]),
    num_node_types=len(metadata[0]),
    heads=2
)

# Example: add random labels for 'user' node type
# (Replace with your actual labels)
num_user_nodes = hg['user'].x.size(0)
hg['user'].y = torch.randint(0, 2, (num_user_nodes,))

# Create train/test masks
train_indices, test_indices = train_test_split(
    torch.arange(num_user_nodes), test_size=0.2, random_state=42
)

hg['user'].train_mask = torch.zeros(num_user_nodes, dtype=torch.bool)
hg['user'].test_mask = torch.zeros(num_user_nodes, dtype=torch.bool)

hg['user'].train_mask[train_indices] = True
hg['user'].test_mask[test_indices] = True

#########################
# Define loss and optimizer
#########################
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)

#########################
# Training and Testing
#########################
def train():
    model.train()
    optimizer.zero_grad()
    out_dict = model(hg)
    out = out_dict['user']
    train_mask = hg['user'].train_mask
    loss = criterion(out[train_mask], hg['user'].y[train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

@torch.no_grad()
def test():
    model.eval()
    out_dict = model(hg)
    out = out_dict['user']
    pred = out.argmax(dim=1)
    train_mask = hg['user'].train_mask
    test_mask = hg['user'].test_mask

    train_acc = (pred[train_mask] == hg['user'].y[train_mask]).sum().item() / train_mask.sum().item()
    test_acc = (pred[test_mask] == hg['user'].y[test_mask]).sum().item() / test_mask.sum().item()
    return train_acc, test_acc

#########################
# Run training
#########################
num_epochs = 100
for epoch in range(1, num_epochs + 1):
    loss = train()
    train_acc, test_acc = test()
    if epoch % 20 == 1 or epoch == num_epochs:
        print(f"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")
'''