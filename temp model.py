# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMLsTkwJado-1yLshxY6VBSwEf-IXS13

#install phase
"""

# !pip install yake

# !pip install fasttext

"""# imports"""

import json
import numpy as np
import random
import networkx as nx
from matplotlib import pyplot as plt
from tqdm import tqdm
import pickle

import yake

from gensim.models.fasttext import load_facebook_vectors
from sklearn.decomposition import PCA

SEED = 1

"""# data creation"""
'''
def read_json_to_dict(json_filepath):
    with open(json_filepath, "r", encoding="utf-8") as file:
        data = json.load(file)
    return data

json_filepath = "people_tweets.json"
people_tweets_dict = read_json_to_dict(json_filepath)

for row in people_tweets_dict:
    row['person_id'] = row['person_id'] - 1

people_tweets_dict[:2]

num_nodes = 100  # Number of nodes
num_edges = 3  # Number of edges to attach from a new node to existing nodes

# Create the undirected BA graph
ba_graph = nx.barabasi_albert_graph(num_nodes, num_edges, seed=42)

# Convert to a directed graph
userG = nx.DiGraph()
for u, v in ba_graph.edges():
    if random.choice([True, False]):  # Randomly assign direction
        userG.add_edge(u, v)
    else:
        userG.add_edge(v, u)

# Draw the directed graph
plt.figure(figsize=(10, 8))
nx.draw(userG, node_color="lightblue", edge_color="gray", node_size=50, arrows=True)
plt.title("Directed Barab√°si-Albert Graph")
plt.show()

# Print the number of nodes and edges
print(f"Number of nodes: {userG.number_of_nodes()}")
print(f"Number of edges: {userG.number_of_edges()}")

random.seed(SEED)
nodes = list(userG.nodes())
train_benign = random.sample(nodes, 10)
remaining_nodes = list(set(nodes) - set(train_benign))
test_benign = random.sample(remaining_nodes, 10)
remaining_nodes = list(set(remaining_nodes) - set(test_benign))
train_sybil = random.sample(remaining_nodes, 10)
remaining_nodes = list(set(remaining_nodes) - set(train_sybil))
test_sybil = random.sample(remaining_nodes, 10)

# Assign labels
train_benign_labels = {node: 0 for node in train_benign}
test_benign_labels = {node: 0 for node in test_benign}
train_sybil_labels = {node: 1 for node in train_sybil}
test_sybil_labels = {node: 1 for node in test_sybil}

"""# Keywords"""

number_of_keywords = 20

yake_extractor = yake.KeywordExtractor(lan="en", n=1, dedupLim=0.9)

def extract_keywords_with_yake(tweets, top_n=10):
    """Extract top_n keywords using YAKE for a list of tweets."""
    combined_text = " ".join(tweets)
    keywords = yake_extractor.extract_keywords(combined_text)
    return [kw[0] for kw in keywords[:top_n]]

# File paths
keywords_per_tweet_file = 'user_keywords.txt'
keywords_set_file = 'all_keywords.txt'

# Dictionaries to store data
tweet_keywords = {}
all_keywords = set()

keywords_users_dic = {}
ts_id = 0
for row in tqdm(people_tweets_dict):
    ts = row['tweets']
    user = row['person_id']
    keywords_users_dic[user] = []
    for tweet in ts:
        keywords = [kw[0] for kw in yake_extractor.extract_keywords(tweet)[:number_of_keywords]]
        keywords_users_dic[user].append({'t'+str(ts_id):keywords[:]})
        ts_id+=1
        all_keywords.update(keywords[:])

"""# creat graph"""

HG = nx.MultiDiGraph()

# Add nodes of different types include 'user', 'keyword', 'tweet'
for node in list(userG.nodes()):
    HG.add_node(node, node_type="user")
for node in tqdm(all_keywords):
    HG.add_node(node, node_type="keyword")

ts_id = 0
for row in people_tweets_dict:
    ts = row['tweets']
    for t in ts:
        HG.add_node('t'+str(ts_id), node_type="tweet")
        ts_id += 1

# Add directed edges with types include 'follow', 'haskeyword', 'keywordintweet', 'hastweet', 'tweetedby'
for edge in list(userG.edges()):
    HG.add_edge(edge[0], edge[1], edge_type="follow")

for row in people_tweets_dict:
    user = row['person_id']
    for ts in keywords_users_dic[user]:
        ts_id = list(ts.keys())[0]
        tw_keywords = list(ts.values())[0]

        HG.add_edge(user, ts_id, edge_type="hastweet")
        HG.add_edge(ts_id, user, edge_type="tweetedby")

        for kw in tw_keywords:
            HG.add_edge(kw, ts_id, edge_type="keywordintweet")
            HG.add_edge(ts_id, kw, edge_type="haskeyword")



"""#embedding"""

def fasttext_embeddings_with_reduction(model,words, target_size=64):
    # Compute embeddings for all words
    original_embeddings = []
    for word in words:
        embedding = model[word]
        original_embeddings.append(embedding)
    # Perform PCA to reduce embedding size to the target dimension
    pca = PCA(n_components=target_size)
    reduced_embeddings = pca.fit_transform(original_embeddings)
    return {word: reduced_embeddings[i] for i, word in enumerate(words)}



fasttext_model = load_facebook_vectors('cc.en.300.bin')

word_embedding_dictionary = fasttext_embeddings_with_reduction(fasttext_model,all_keywords,16)


with open('word_embedding_dictionary.pickle', 'wb') as handle:
    pickle.dump(word_embedding_dictionary, handle)

with open('HG.pickle', 'wb') as handle:
    pickle.dump(HG, handle)





with open('word_embedding_dictionary.pickle', 'rb') as handle:
    word_embedding_dictionary = pickle.load(handle)

with open('HG.pickle', 'rb') as handle:
    HG = pickle.load(handle)
    



# Assign embeddings to keyword nodes
for node, data in HG.nodes(data=True):
    if data["node_type"] == "keyword":
        keyword = node
        data["embedding"] = word_embedding_dictionary[keyword]

# Assign embeddings to tweet nodes
for node, data in HG.nodes(data=True):
    if data["node_type"] == "tweet":
        keywords = [
            nbr for nbr, attr in HG[node].items() if attr.get("edge_type") == "haskeyword"
        ]
        keyword_embeddings = [
            HG.nodes[keyword]["embedding"] for keyword in keywords if "embedding" in HG.nodes[keyword]
        ]
        if keyword_embeddings:
            avg_embedding = np.mean(keyword_embeddings, axis=0)
            data["embedding"] = avg_embedding + np.random.normal(scale=0.01, size=avg_embedding.shape)
        else:
            data["embedding"] = np.random.rand(16)  # Default random embedding

# Assign embeddings to user nodes
for node, data in HG.nodes(data=True):
    if data["node_type"] == "user":
        tweets = [
            nbr for nbr, attr in HG[node].items() if attr.get("edge_type") == "hastweet"
        ]
        tweet_embeddings = [
            HG.nodes[tweet]["embedding"] for tweet in tweets if "embedding" in HG.nodes[tweet]
        ]
        if tweet_embeddings:
            avg_embedding = np.mean(tweet_embeddings, axis=0)
            data["embedding"] = avg_embedding + np.random.normal(scale=0.01, size=avg_embedding.shape)
        else:
            data["embedding"] = np.random.rand(16)  # Default random embedding
            

for node, data in HG.nodes(data=True):
    print(f"Node: {node}, Type: {data['node_type']}, Embedding Shape: {data['embedding'].shape}")
    
def relabel_heterogeneous_graph(HG):
    # Extract nodes and create a mapping
    old_to_new = {node: idx for idx, node in enumerate(HG.nodes())}
    new_to_old = {idx: node for node, idx in old_to_new.items()}
    
    # Relabel nodes in the graph
    HG_relabelled = nx.relabel_nodes(HG, old_to_new)
    
    return HG_relabelled, old_to_new, new_to_old

HG, old_to_new, new_to_old = relabel_heterogeneous_graph(HG)




with open('HG.pickle', 'wb') as handle:
    pickle.dump(HG, handle)

 

'''

'''
with open('HG.pickle', 'rb') as handle:
    HG = pickle.load(handle)
    
    

import torch
from torch_geometric.nn import HGTConv
from torch_geometric.data import HeteroData


# 'user', 'keyword', 'tweet'
# 'follow', 'haskeyword', 'keywordintweet', 'hastweet', 'tweetedby'

node_mapping = {node: idx for idx, node in tqdm(enumerate(HG.nodes))}

# Initialize HeteroData object
data = HeteroData()

from collections import defaultdict
node_features = defaultdict(list)  # Temporary storage for node features

# Collect node embeddings grouped by node_type
for node, attributes in HG.nodes(data=True):
    node_type = attributes.get('node_type')  # Get node type
    embedding = attributes.get('embedding')  # Get embedding

    if node_type is None or embedding is None:
        print(f"Node {node} is missing 'node_type' or 'embedding'. Skipping.")
        continue

    node_features[node_type].append(embedding)  # Group embeddings by node_type

# Convert grouped embeddings to tensors and assign to HeteroData
for node_type, embeddings in node_features.items():
    # Convert list of embeddings to tensor
    data[node_type].x = torch.tensor(embeddings, dtype=torch.float)


# Convert node features (embeddings) to tensor
for node_type in data.node_types:
    data[node_type].x = torch.tensor(data[node_type].x, dtype=torch.float)

# Add edges to HeteroData based on their type
for source, target, attributes in HG.edges(data=True):
    edge_type = attributes['edge_type']
    src_type = HG.nodes[source]['node_type']
    tgt_type = HG.nodes[target]['node_type']
    if (src_type, edge_type, tgt_type) not in data.edge_types:
        data[(src_type, edge_type, tgt_type)].edge_index = [[], []]
    data[(src_type, edge_type, tgt_type)].edge_index[0].append(node_mapping[source])
    data[(src_type, edge_type, tgt_type)].edge_index[1].append(node_mapping[target])

# Convert edge indices to tensors
for edge_type in data.edge_types:
    edge_data = data[edge_type]
    edge_data.edge_index = torch.tensor(edge_data.edge_index, dtype=torch.long).T

# The HeteroData object is now populated with embeddings
print(data)

with open('data.pickle', 'wb') as handle:
    pickle.dump(data, handle)


'''
with open('data.pickle', 'rb') as handle:
    data = pickle.load(handle)
    
import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv, HeteroConv

# Define a HeteroGAT model for node classification
class HeteroGAT(torch.nn.Module):
    def __init__(self, metadata, hidden_dim, out_dim):
        super().__init__()
        # Heterogeneous convolution layer
        self.conv1 = HeteroConv(
            {
                edge_type: GATConv((-1, -1), hidden_dim, add_self_loops=False)
                for edge_type in metadata[1]  # metadata[1] contains edge types
            },
            aggr='mean',  # Aggregate messages
        )
        self.conv2 = HeteroConv(
            {
                edge_type: GATConv((-1, -1), out_dim, add_self_loops=False)
                for edge_type in metadata[1]
            },
            aggr='mean',
        )

    def forward(self, x_dict, edge_index_dict):
        # First layer
        x_dict = self.conv1(x_dict, edge_index_dict)
        x_dict = {key: F.relu(x) for key, x in x_dict.items()}  # Activation

        # Second layer
        x_dict = self.conv2(x_dict, edge_index_dict)
        return x_dict

# Initialize the model
metadata = data.metadata()  # Get metadata (node types, edge types)
model = HeteroGAT(metadata, hidden_dim=16, out_dim=2)  # Example: 2 output classes

# Example: Add labels and train/test masks
data['user'].y = torch.tensor([0, 1, 0], dtype=torch.long)  # Example labels
data['user'].train_mask = torch.tensor([True, True, False], dtype=torch.bool)
data['user'].test_mask = torch.tensor([False, False, True], dtype=torch.bool)


random.seed(SEED)
# Assuming 100 nodes in the 'user' type
num_nodes = 100
data['user'].y = torch.tensor([0 if i < 50 else 1 for i in range(num_nodes)], dtype=torch.long)  # 0: benign, 1: sybil

# Randomly select nodes for each category
benign_indices = [i for i in range(50)]  # First 50 nodes are benign
sybil_indices = [i for i in range(50, 100)]  # Next 50 nodes are sybil

# Shuffle indices to randomize selection
random.shuffle(benign_indices)
random.shuffle(sybil_indices)

# Assign nodes for train and test
train_benign = benign_indices[:10]
train_sybil = sybil_indices[:10]
test_benign = benign_indices[10:20]
test_sybil = sybil_indices[10:20]

# Combine indices
train_indices = train_benign + train_sybil
test_indices = test_benign + test_sybil

# Create masks
train_mask = [i in train_indices for i in range(num_nodes)]
test_mask = [i in test_indices for i in range(num_nodes)]

# Add masks to the data object
data['user'].train_mask = torch.tensor(train_mask, dtype=torch.bool)
data['user'].test_mask = torch.tensor(test_mask, dtype=torch.bool)

# Verify the distribution
# print("Train Mask:", data['user'].train_mask.nonzero(as_tuple=True)[0])
# print("Test Mask:", data['user'].test_mask.nonzero(as_tuple=True)[0])
# print("Labels (Train):", data['user'].y[data['user'].train_mask])
# print("Labels (Test):", data['user'].y[data['user'].test_mask])


# Optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
criterion = torch.nn.CrossEntropyLoss()





new_edge_index_dict = {}
for edge_type, edge_index in data.edge_index_dict.items():
    # Transpose the edge_index and store in the new dictionary
    new_edge_index_dict[edge_type] = edge_index.T.clone()
    print(f"Edge type {edge_type}: Transposed shape = {new_edge_index_dict[edge_type].shape}")




combined_tensor = torch.cat(list(data.x_dict.values()), dim=0)

print(combined_tensor.type)
 
# Training loop
for epoch in range(50):  # Number of epochs
    model.train()
    optimizer.zero_grad()

    # Forward pass
    
    out = model(combined_tensor, new_edge_index_dict)

    # Compute loss for the 'user' node type
    train_mask = data['user'].train_mask
    loss = criterion(out['user'][train_mask], data['user'].y[train_mask])

    # Backward pass
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch + 1}, Loss: {loss.item()}")

# Evaluate on test set
model.eval()
test_mask = data['user'].test_mask
pred = out['user'][test_mask].argmax(dim=1)
acc = (pred == data['user'].y[test_mask]).sum() / test_mask.sum()
print(f"Test Accuracy: {acc:.4f}")

