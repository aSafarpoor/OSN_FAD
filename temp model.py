# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMLsTkwJado-1yLshxY6VBSwEf-IXS13

#install phase
"""

# !pip install yake

# !pip install fasttext

"""# imports"""

import json
import numpy as np
import random
import networkx as nx
from matplotlib import pyplot as plt
from tqdm import tqdm
import pickle

import yake

from gensim.models.fasttext import load_facebook_vectors
from sklearn.decomposition import PCA

SEED = 1

"""# data creation"""
'''
def read_json_to_dict(json_filepath):
    with open(json_filepath, "r", encoding="utf-8") as file:
        data = json.load(file)
    return data

json_filepath = "people_tweets.json"
people_tweets_dict = read_json_to_dict(json_filepath)

for row in people_tweets_dict:
    row['person_id'] = row['person_id'] - 1

people_tweets_dict[:2]

num_nodes = 100  # Number of nodes
num_edges = 3  # Number of edges to attach from a new node to existing nodes

# Create the undirected BA graph
ba_graph = nx.barabasi_albert_graph(num_nodes, num_edges, seed=42)

# Convert to a directed graph
userG = nx.DiGraph()
for u, v in ba_graph.edges():
    if random.choice([True, False]):  # Randomly assign direction
        userG.add_edge(u, v)
    else:
        userG.add_edge(v, u)

# Draw the directed graph
plt.figure(figsize=(10, 8))
nx.draw(userG, node_color="lightblue", edge_color="gray", node_size=50, arrows=True)
plt.title("Directed Barab√°si-Albert Graph")
plt.show()

# Print the number of nodes and edges
print(f"Number of nodes: {userG.number_of_nodes()}")
print(f"Number of edges: {userG.number_of_edges()}")

random.seed(SEED)
nodes = list(userG.nodes())
train_benign = random.sample(nodes, 10)
remaining_nodes = list(set(nodes) - set(train_benign))
test_benign = random.sample(remaining_nodes, 10)
remaining_nodes = list(set(remaining_nodes) - set(test_benign))
train_sybil = random.sample(remaining_nodes, 10)
remaining_nodes = list(set(remaining_nodes) - set(train_sybil))
test_sybil = random.sample(remaining_nodes, 10)

# Assign labels
train_benign_labels = {node: 0 for node in train_benign}
test_benign_labels = {node: 0 for node in test_benign}
train_sybil_labels = {node: 1 for node in train_sybil}
test_sybil_labels = {node: 1 for node in test_sybil}

"""# Keywords"""

number_of_keywords = 20

yake_extractor = yake.KeywordExtractor(lan="en", n=1, dedupLim=0.9)

def extract_keywords_with_yake(tweets, top_n=10):
    """Extract top_n keywords using YAKE for a list of tweets."""
    combined_text = " ".join(tweets)
    keywords = yake_extractor.extract_keywords(combined_text)
    return [kw[0] for kw in keywords[:top_n]]

# File paths
keywords_per_tweet_file = 'user_keywords.txt'
keywords_set_file = 'all_keywords.txt'

# Dictionaries to store data
tweet_keywords = {}
all_keywords = set()

keywords_users_dic = {}
ts_id = 0
for row in tqdm(people_tweets_dict):
    ts = row['tweets']
    user = row['person_id']
    keywords_users_dic[user] = []
    for tweet in ts:
        keywords = [kw[0] for kw in yake_extractor.extract_keywords(tweet)[:number_of_keywords]]
        keywords_users_dic[user].append({'t'+str(ts_id):keywords[:]})
        ts_id+=1
        all_keywords.update(keywords[:])

"""# creat graph"""

HG = nx.MultiDiGraph()

# Add nodes of different types include 'user', 'keyword', 'tweet'
for node in list(userG.nodes()):
    HG.add_node(node, node_type="user")
for node in tqdm(all_keywords):
    HG.add_node(node, node_type="keyword")

ts_id = 0
for row in people_tweets_dict:
    ts = row['tweets']
    for t in ts:
        HG.add_node('t'+str(ts_id), node_type="tweet")
        ts_id += 1

# Add directed edges with types include 'follow', 'haskeyword', 'keywordintweet', 'hastweet', 'tweetedby'
for edge in list(userG.edges()):
    HG.add_edge(edge[0], edge[1], edge_type="follow")

for row in people_tweets_dict:
    user = row['person_id']
    for ts in keywords_users_dic[user]:
        ts_id = list(ts.keys())[0]
        tw_keywords = list(ts.values())[0]

        HG.add_edge(user, ts_id, edge_type="hastweet")
        HG.add_edge(ts_id, user, edge_type="tweetedby")

        for kw in tw_keywords:
            HG.add_edge(kw, ts_id, edge_type="keywordintweet")
            HG.add_edge(ts_id, kw, edge_type="haskeyword")



"""#embedding"""

def fasttext_embeddings_with_reduction(model,words, target_size=64):
    # Compute embeddings for all words
    original_embeddings = []
    for word in words:
        embedding = model[word]
        original_embeddings.append(embedding)
    # Perform PCA to reduce embedding size to the target dimension
    pca = PCA(n_components=target_size)
    reduced_embeddings = pca.fit_transform(original_embeddings)
    return {word: reduced_embeddings[i] for i, word in enumerate(words)}



fasttext_model = load_facebook_vectors('cc.en.300.bin')

word_embedding_dictionary = fasttext_embeddings_with_reduction(fasttext_model,all_keywords,16)


with open('word_embedding_dictionary.pickle', 'wb') as handle:
    pickle.dump(word_embedding_dictionary, handle)

with open('HG.pickle', 'wb') as handle:
    pickle.dump(HG, handle)





with open('word_embedding_dictionary.pickle', 'rb') as handle:
    word_embedding_dictionary = pickle.load(handle)

with open('HG.pickle', 'rb') as handle:
    HG = pickle.load(handle)
    



# Assign embeddings to keyword nodes
for node, data in HG.nodes(data=True):
    if data["node_type"] == "keyword":
        keyword = node
        data["embedding"] = word_embedding_dictionary[keyword]

# Assign embeddings to tweet nodes
for node, data in HG.nodes(data=True):
    if data["node_type"] == "tweet":
        keywords = [
            nbr for nbr, attr in HG[node].items() if attr.get("edge_type") == "haskeyword"
        ]
        keyword_embeddings = [
            HG.nodes[keyword]["embedding"] for keyword in keywords if "embedding" in HG.nodes[keyword]
        ]
        if keyword_embeddings:
            avg_embedding = np.mean(keyword_embeddings, axis=0)
            data["embedding"] = avg_embedding + np.random.normal(scale=0.01, size=avg_embedding.shape)
        else:
            data["embedding"] = np.random.rand(16)  # Default random embedding

# Assign embeddings to user nodes
for node, data in HG.nodes(data=True):
    if data["node_type"] == "user":
        tweets = [
            nbr for nbr, attr in HG[node].items() if attr.get("edge_type") == "hastweet"
        ]
        tweet_embeddings = [
            HG.nodes[tweet]["embedding"] for tweet in tweets if "embedding" in HG.nodes[tweet]
        ]
        if tweet_embeddings:
            avg_embedding = np.mean(tweet_embeddings, axis=0)
            data["embedding"] = avg_embedding + np.random.normal(scale=0.01, size=avg_embedding.shape)
        else:
            data["embedding"] = np.random.rand(16)  # Default random embedding
            

for node, data in HG.nodes(data=True):
    print(f"Node: {node}, Type: {data['node_type']}, Embedding Shape: {data['embedding'].shape}")
    
def relabel_heterogeneous_graph(HG):
    # Extract nodes and create a mapping
    old_to_new = {node: idx for idx, node in enumerate(HG.nodes())}
    new_to_old = {idx: node for node, idx in old_to_new.items()}
    
    # Relabel nodes in the graph
    HG_relabelled = nx.relabel_nodes(HG, old_to_new)
    
    return HG_relabelled, old_to_new, new_to_old

HG, old_to_new, new_to_old = relabel_heterogeneous_graph(HG)




with open('HG.pickle', 'wb') as handle:
    pickle.dump(HG, handle)

 

'''

'''
with open('HG.pickle', 'rb') as handle:
    HG = pickle.load(handle)

def update_heterogeneous_graph(HG, node_mapping):
    # Create a new graph to hold updated nodes
    updated_HG = nx.Graph() if isinstance(HG, nx.Graph) else nx.DiGraph()

    # Update nodes and their attributes
    for node, data in tqdm(HG.nodes(data=True)):
        new_node = node_mapping[node]
        updated_HG.add_node(new_node, **data)  # Copy node attributes to the new graph

    # Add edges with the new node identifiers
    for u, v, data in tqdm(HG.edges(data=True)):
        updated_HG.add_edge(node_mapping[u], node_mapping[v], **data)

    return updated_HG

node_mapping = {node: idx for idx, node in tqdm(enumerate(HG.nodes))}
HG = update_heterogeneous_graph(HG, node_mapping)

with open('HG.pickle', 'wb') as handle:
    pickle.dump(HG, handle)

'''

'''

with open('HG.pickle', 'rb') as handle:
    HG = pickle.load(handle)
    
    

import torch
from torch_geometric.nn import HGTConv
from torch_geometric.data import HeteroData


# 'user', 'keyword', 'tweet'
# 'follow', 'haskeyword', 'keywordintweet', 'hastweet', 'tweetedby'


# Initialize HeteroData object
data = HeteroData()

from collections import defaultdict
node_features = defaultdict(list)  # Temporary storage for node features

# Collect node embeddings grouped by node_type
for node, attributes in HG.nodes(data=True):
    node_type = attributes.get('node_type')  # Get node type
    embedding = attributes.get('embedding')  # Get embedding

    if node_type is None or embedding is None:
        print(f"Node {node} is missing 'node_type' or 'embedding'. Skipping.")
        continue

    node_features[node_type].append(embedding)  # Group embeddings by node_type

# Convert grouped embeddings to tensors and assign to HeteroData
for node_type, embeddings in node_features.items():
    # Convert list of embeddings to tensor
    data[node_type].x = torch.tensor(embeddings, dtype=torch.float)


# Convert node features (embeddings) to tensor
for node_type in data.node_types:
    data[node_type].x = torch.tensor(data[node_type].x, dtype=torch.float)

# Add edges to HeteroData based on their type
for source, target, attributes in HG.edges(data=True):
    edge_type = attributes['edge_type']
    src_type = HG.nodes[source]['node_type']
    tgt_type = HG.nodes[target]['node_type']

    # Initialize the edge_index for this edge type if it doesn't exist
    if (src_type, edge_type, tgt_type) not in data.edge_types:
        data[(src_type, edge_type, tgt_type)].edge_index = torch.empty((2, 0), dtype=torch.long)

    # Append the edge indices to the existing tensor
    current_edges = data[(src_type, edge_type, tgt_type)].edge_index
    new_edges = torch.tensor([[source], [target]], dtype=torch.long)
    data[(src_type, edge_type, tgt_type)].edge_index = torch.cat([current_edges, new_edges], dim=1)

# The HeteroData object is now populated with tensors
print(data)

# Verify the types of edge_index
# print(type(data['user', 'follow', 'user'].edge_index))  # Should be torch.Tensor
# print(type(data['user', 'follow', 'user'].edge_index[0]))


with open('data.pickle', 'wb') as handle:
    pickle.dump(data, handle)


'''



with open('data.pickle', 'rb') as handle:
    data = pickle.load(handle)
    
print(data)
# user_node_data = data['user']

import torch
import torch.nn.functional as F
from torch_geometric.nn import HeteroConv, GATConv, Linear
from sklearn.model_selection import train_test_split


class HeteroGNNWithTypes(torch.nn.Module):
    def __init__(self, metadata, in_channels, hidden_channels, out_channels, num_edge_types, num_node_types, heads=1):
        super().__init__()
        self.metadata = metadata

        # Initialize the GNN layers for edge types
        self.conv1 = HeteroConv({
            edge_type: GATConv(
                in_channels=in_channels,
                out_channels=hidden_channels,
                heads=heads,
                add_self_loops = False
            )
            for edge_type in metadata[1]
        }, aggr='mean')  # Aggregate results across edge types

        self.conv2 = HeteroConv({
            edge_type: GATConv(
                in_channels=hidden_channels * heads,
                out_channels=hidden_channels,
                heads=heads,
                add_self_loops = False
            )
            for edge_type in metadata[1]
        }, aggr='mean')

        # Final classifier for node types
        self.classifiers = torch.nn.ModuleDict({
            node_type: Linear(hidden_channels * heads, out_channels)
            for node_type in metadata[0]
        })

        # Embeddings for node and edge types
        self.node_type_embed = torch.nn.Embedding(num_node_types, in_channels)
        self.edge_type_embed = torch.nn.Embedding(num_edge_types, hidden_channels)

    def forward(self, data):
        x_dict, edge_index_dict = {}, {}

        # Prepare x_dict and add node type embeddings
        for node_type in data.node_types:
            x_dict[node_type] = data[node_type].x
            node_type_ids = torch.full((x_dict[node_type].size(0),), list(data.node_types).index(node_type), dtype=torch.long)
            x_dict[node_type] += self.node_type_embed(node_type_ids)

        # Prepare edge_index_dict without self-loops
        for edge_type in data.edge_types:
            edge_index = data[edge_type].edge_index
            edge_index_dict[edge_type] = edge_index  # Use edge_index as-is without adding self-loops



        node_offsets = {}
        current_offset = 0
        for node_type in metadata[0]:
            node_offsets[node_type] = current_offset
            current_offset += data[node_type]['x'].size(0)

        # Adjust edge indices
        adjusted_edge_index_dict = {}
        for edge_type in data.edge_types:
            src_type, _, tgt_type = edge_type
            edge_index = data[edge_type]['edge_index']

            # Adjust indices for source and target nodes
            adjusted_edge_index = edge_index.clone()  # Copy the original edge_index
            adjusted_edge_index[0] -= node_offsets[src_type]  # Adjust source node indices
            adjusted_edge_index[1] -= node_offsets[tgt_type]  # Adjust target node indices

            # Store the adjusted edge index
            adjusted_edge_index_dict[edge_type] = adjusted_edge_index


        # Pass through first HeteroConv layer
        x_dict = self.conv1(x_dict, adjusted_edge_index_dict)
        x_dict = {key: F.elu(x) for key, x in x_dict.items()}

        # Pass through second HeteroConv layer
        x_dict = self.conv2(x_dict, adjusted_edge_index_dict)
        x_dict = {key: F.elu(x) for key, x in x_dict.items()}

        # Apply classifiers for each node type
        out_dict = {}
        for node_type, x in x_dict.items():
            out_dict[node_type] = F.softmax(self.classifiers[node_type](x), dim=1)

        return out_dict


# Metadata for the heterogeneous graph
metadata = (
    ['user', 'keyword', 'tweet'],  # Node types
    [('user', 'follow', 'user'), ('user', 'hastweet', 'tweet'), ('tweet', 'tweetedby', 'user'), ('keyword', 'keywordintweet', 'tweet'), ('tweet', 'haskeyword', 'keyword') ]  # Edge types
)

# Initialize the model
model = HeteroGNNWithTypes(
    metadata=metadata,
    in_channels=16,  # Match input feature size
    hidden_channels=32,  # Hidden size
    out_channels=2,  # Binary classification
    num_edge_types=len(metadata[1]),  # Number of edge types
    num_node_types=len(metadata[0]),  # Number of node types
    heads=2  # Multi-head attention
)

# Forward pass
output = model(data)
# Print output
for node_type, logits in output.items():
    print(f"{node_type} output shape: {logits.shape}")

# Example data structure for node labels
# Replace with your actual node labels for classification
data['user'].y = torch.randint(0, 2, (100,))  # Random binary labels for users

# Split indices for training and testing
train_mask, test_mask = train_test_split(
    torch.arange(data['user'].x.size(0)), test_size=0.2, random_state=42
)

# Create train/test masks
data['user'].train_mask = torch.zeros(data['user'].x.size(0), dtype=torch.bool)
data['user'].test_mask = torch.zeros(data['user'].x.size(0), dtype=torch.bool)

data['user'].train_mask[train_mask] = True
data['user'].test_mask[test_mask] = True

# Define loss and optimizer
criterion = torch.nn.CrossEntropyLoss()  # Suitable for multi-class classification
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)

# Training loop
def train():
    model.train()  # Set model to training mode
    optimizer.zero_grad()  # Clear previous gradients

    # Forward pass
    out_dict = model(data)
    out = out_dict['user']  # Focus on the target node type

    # Compute loss only for training nodes
    train_mask = data['user'].train_mask
    loss = criterion(out[train_mask], data['user'].y[train_mask])

    # Backward pass and optimization
    loss.backward(retain_graph=True )  # Compute gradients
    optimizer.step()  # Update model parameters

    return loss.item()

# Testing function
@torch.no_grad()
def test():
    model.eval()  # Set model to evaluation mode

    # Forward pass
    out_dict = model(data)
    out = out_dict['user']

    # Get predictions
    pred = out.argmax(dim=1)  # Predicted class labels

    # Evaluate accuracy on train and test sets
    train_mask = data['user'].train_mask
    test_mask = data['user'].test_mask

    train_acc = (pred[train_mask] == data['user'].y[train_mask]).sum().item() / train_mask.sum().item()
    test_acc = (pred[test_mask] == data['user'].y[test_mask]).sum().item() / test_mask.sum().item()

    return train_acc, test_acc

# Training loop
num_epochs = 100
for epoch in range(1, num_epochs + 1):
    loss = train()
    train_acc, test_acc = test()

    if epoch%20==1 or epoch == num_epochs:
        print(f"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")
